# -*- coding: utf-8 -*-
"""NLP Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_qFhFoFw2p7pmwj0UcYzKBylTfNDZBrP

# Customer Reviews Analysis and Topic Modeling

In this project, we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures.

## Contents

* [Part 1: Load Data](#Part-1:-Load-Data)
* [Part 2: Tokenizing and Stemming](#Part-2:-Tokenizing-and-Stemming)
* [Part 3: TF-IDF](#Part-3:-TF-IDF)
* [Part 4: K-means clustering](#Part-4:-K-means-clustering)
* [Part 5: Topic Modeling - Latent Dirichlet Allocation](#Part-5:-Topic-Modeling---Latent-Dirichlet-Allocation)

# Part 0: Setup Google Drive Environment
"""

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file = drive.CreateFile({'id':'1zDZlbvBFl4pEEsNTekfeJpXPf8Ci0Gyx'}) # replace the id with id of file you want to access
file.GetContentFile('watch_reviews.tsv')  # tab-separated

"""# Part 1: Load Data"""

import numpy as np
import pandas as pd
import nltk
# import gensim

from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('stopwords')

# Load data into dataframe
df = pd.read_csv('watch_reviews.tsv', sep='\t', error_bad_lines=False) #If False, then these “bad lines” will dropped from the DataFrame that is returned

df.head()



# Remove missing value
# subset: Define in which columns to look for missing values
# When inplace = True , the data is modified in place, which means it will return nothing and the dataframe is now updated. 
# When inplace = False , which is the default, then the operation is performed and it returns a copy of the object.

df.dropna(subset=['review_body'],inplace=True)

# after removing missing values, reset the index
df.reset_index(inplace=True, drop=True)

df.info()

# use the first 1000 data as our training data, Return a list of the values.
data = df.loc[:999, 'review_body'].tolist()

data

"""# Part 2: Tokenizing and Stemming

Load stopwords and stemmer function from NLTK library.
Stop words are words like "a", "the", or "in" which don't convey significant meaning.
Stemming is the process of breaking a word down into its root.
"""

# Use nltk's English stopwords.
stopwords = nltk.corpus.stopwords.words('english') #stopwords.append("n't")
stopwords.append("'s")
stopwords.append("'m")
stopwords.append("br") #html <br>
stopwords.append("watch")

print ("We use " + str(len(stopwords)) + " stop-words from nltk library.")
print (stopwords[:10])

"""Use our defined functions to analyze (i.e. tokenize, stem) our reviews."""

from nltk.stem.snowball import SnowballStemmer
# from nltk.stem import WordNetLemmatizer 

stemmer = SnowballStemmer("english")

# tokenization and stemming
def tokenization_and_stemming(text):
    tokens = []
    # exclude stop words and tokenize the document, generate a list of string 
    for word in nltk.word_tokenize(text):
        if word.lower() not in stopwords:
            tokens.append(word.lower())

    filtered_tokens = []
    
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if token.isalpha(): #Check if all the characters in the text are letters
            filtered_tokens.append(token)
            
    # stemming
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems

tokenization_and_stemming(data[0])

data[0]

"""# Part 3: TF-IDF

TF: Term Frequency

IDF: Inverse Document Frequency
"""

from sklearn.feature_extraction.text import TfidfVectorizer
# define vectorizer parameters
# TfidfVectorizer will help us to create tf-idf matrix
# max_df : maximum document frequency for the given word
# min_df : minimum document frequency for the given word
# max_features: maximum number of words
# use_idf: if not true, we only calculate tf
# stop_words : built-in stop words
# tokenizer: how to tokenize the document
# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram
tfidf_model = TfidfVectorizer(max_df=0.99, max_features=1000,
                                 min_df=0.01, stop_words='english',
                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))

tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses

print ("In total, there are " + str(tfidf_matrix.shape[0]) + \
      " reviews and " + str(tfidf_matrix.shape[1]) + " terms.").  # 1000 documents, 239 unique words/features

tfidf_matrix

tfidf_matrix.toarray() #todense()

tfidf_matrix.todense()

print(type(tfidf_matrix.toarray()))

print(type(tfidf_matrix.todense()))

"""Save the terms identified by TF-IDF."""

# words
tf_selected_words = tfidf_model.get_feature_names()

# print out words
tf_selected_words

"""# Part 4: K-means clustering"""

# k-means clustering
from sklearn.cluster import KMeans

num_clusters = 5

# number of clusters
km = KMeans(n_clusters=num_clusters)
km.fit(tfidf_matrix)

clusters = km.labels_.tolist()

"""## 4.1. Analyze K-means Result"""

# create DataFrame films from all of the input files.
product = { 'review': df[:1000].review_body, 'cluster': clusters} # initialize data of lists.
frame = pd.DataFrame(product, columns = ['review', 'cluster'])  # Create DataFrame

frame.head(10)

print ("Number of reviews included in each cluster:")
frame['cluster'].value_counts().to_frame() #Return a Series containing counts of unique values. Convert Series to DataFrame

km.cluster_centers_

# 239数的list -> cluster 0的中心点的tf-idf值
#-> assumption: 中心点的值可以代表这个cluster
#-> tf-idf值越大，对应的词越能代表这个document; The larger the tF-IDF value is, the more the corresponding word can represent the document
#-> 选出了tf-idf最大的6个值对应的词来代表这个cluster; The words that have the largest six values of TF-IDF are selected to represent this cluster

km.cluster_centers_.shape

print ("<Document clustering result by K-means>")

#km.cluster_centers_ denotes the importances of each items in centroid.
#We need to sort it in decreasing-order and get the top k items.
order_centroids = km.cluster_centers_.argsort()[:, ::-1] #argsort(): Returns the indices that would sort an array

Cluster_keywords_summary = {}
for i in range(num_clusters):
    print ("Cluster " + str(i) + " words:", end='')
    Cluster_keywords_summary[i] = []
    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster
        Cluster_keywords_summary[i].append(tf_selected_words[ind])
        print (tf_selected_words[ind] + ",", end='')
    print ()
    
    cluster_reviews = frame[frame.cluster==i].review.tolist()
    print ("Cluster " + str(i) + " reviews (" + str(len(cluster_reviews)) + " reviews): ")
    print (", ".join(cluster_reviews))
    print ()

"""# Part 5: Topic Modeling - Latent Dirichlet Allocation"""

# Use LDA for clustering
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=5)

# document topic matrix for tfidf_matrix_lda ;the proportion of topic t in document d.
lda_output = lda.fit_transform(tfidf_matrix)
print(lda_output.shape)
print(lda_output)

# topics and words matrix ; the proportion of word w in topic t
topic_word = lda.components_
print(topic_word.shape)
print(topic_word)

# column names
topic_names = ["Topic" + str(i) for i in range(lda.n_components)]

# index names
doc_names = ["Doc" + str(i) for i in range(len(data))]

df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)

# get dominant topic for each document
topic = np.argmax(df_document_topic.values, axis=1) #Returns the indices of the maximum values along an axis.
df_document_topic['topic'] = topic

df_document_topic.head(10)

df_document_topic['topic'].value_counts().to_frame()

# topic word matrix
print(lda.components_)
# topic-word matrix
df_topic_words = pd.DataFrame(lda.components_)

# column and index
df_topic_words.columns = tfidf_model.get_feature_names()
df_topic_words.index = topic_names

df_topic_words.head()

# print top n keywords for each topic
def print_topic_words(tfidf_model, lda_model, n_words):
    words = np.array(tfidf_model.get_feature_names())
    topic_words = []
    # for each topic, we have words weight
    for topic_words_weights in lda_model.components_:
        top_words = topic_words_weights.argsort()[::-1][:n_words]
        topic_words.append(words.take(top_words))
    return topic_words

topic_keywords = print_topic_words(tfidf_model=tfidf_model, lda_model=lda, n_words=15)        

df_topic_words = pd.DataFrame(topic_keywords)
df_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]
df_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]
df_topic_words